Хочу поделиться опытом придания Yandex Алисе (внутри умных колонок поддерживающих TTS API) своевольности. Про локальный API колонок уже писали на хабре тут https://habr.com/ru/post/508106/

Идея заключается в том чтобы Алиса реагировала на присутствующих людей. Для этого их необходимо идентифицировать. Так же Алиса в случайное время будет выдавать некоторую «отсебятину». В статье я не буду касаться эфективных способов распознавания лиц, буду использовать самый простой (на мой взгляд) вариант – тренировка модели в Google Teachable Machine. В статье используется модуль для Node-Red который позволяет посылать команды, управлять воспроизведением и отправлять на яндекс станцию текст который она произнесет.
Используемые инструменты:
1.	Node-Red
2.	Python3
3.	OpenCV
4.	Tensorflow Keras
5.	Google Teachable Machine
6.	PostgresSQL
7.	MQTT

![Sheme](/images/sheme.png)

1)	Скрипт распознает лица и отправляет процент совпадения для каждой сущности модели в MQTT брокер, а так же сохраняет снимок в директорию из файла конфигурации

2)	Node-Red получает сообщения из MQTT брокера и отправляет Алисе команду что либо сказать в зависимости от времени суток и того как давно она “видела” этого человека. При этом фразы для Алисы, а так же время когда человек последний раз засветился на камеру хранятся в БД Postgres.

# Установка

**Linux:**
```
pip3 install opencv-python tensorflow
```
Возможно портебуется установка дополнительных пакетов
```
sudo apt-get install git python3-dev 
```
Загружаем приложение
```
git clone https://github.com/guinmoon/yandex_alice_has_fun
cd yandex_alice_has_fun 
```
# Запуск

```python3 alice_has_fun.py```